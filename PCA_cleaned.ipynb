{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab93afc",
   "metadata": {},
   "source": [
    "## This notebook shows how PCA helps accelerate data market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf0a512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamarket import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3d3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all seller datasets\n",
    "# read buyer dataste\n",
    "# clean_covariance() is not necessary. Originally to make covariance sparse. Doesn't matter...\n",
    "crime = pd.read_csv(\"crime.csv\")\n",
    "crimedata = agg_dataset()\n",
    "crimedata.load(crime, [], [\"DBN\"], \"crime\")\n",
    "crimedata.find_features()\n",
    "crimedata.remove_redundant_columns()\n",
    "crimedata.standardize_all()\n",
    "crimedata.compute_agg(True)\n",
    "crimedata.clean_covariance()\n",
    "\n",
    "esl = pd.read_csv(\"esl.csv\")\n",
    "esldata = agg_dataset()\n",
    "esldata.load(esl, [], [[\"DBN\",\"Grade\"]], \"esl\")\n",
    "esldata.find_features()\n",
    "esldata.remove_redundant_columns()\n",
    "esldata.standardize_all()\n",
    "esldata.compute_agg(True)\n",
    "esldata.clean_covariance()\n",
    "\n",
    "ap = pd.read_csv(\"ap.csv\")\n",
    "apdata = agg_dataset()\n",
    "apdata.load(ap, [], [\"DBN\"], \"ap\")\n",
    "apdata.find_features()\n",
    "apdata.remove_redundant_columns()\n",
    "apdata.standardize_all()\n",
    "apdata.compute_agg(True)\n",
    "apdata.clean_covariance()\n",
    "\n",
    "survey = pd.read_csv(\"2013_NYC_School_Survey.csv\")\n",
    "surveydata = agg_dataset()\n",
    "surveydata.load(survey, [], [\"DBN\"], \"survey\")\n",
    "surveydata.find_features()\n",
    "surveydata.remove_redundant_columns()\n",
    "surveydata.standardize_all()\n",
    "surveydata.compute_agg(True)\n",
    "surveydata.clean_covariance()\n",
    "\n",
    "base = pd.read_csv(\"base.csv\")\n",
    "basedata = agg_dataset()\n",
    "basedata.load(base, [], [\"DBN\"], \"base\")\n",
    "basedata.find_features()\n",
    "basedata.remove_redundant_columns()\n",
    "basedata.standardize_all()\n",
    "basedata.compute_agg(True)\n",
    "basedata.clean_covariance()\n",
    "\n",
    "disc = pd.read_csv(\"disc.csv\")\n",
    "discdata = agg_dataset()\n",
    "discdata.load(disc, [], [\"DBN\"], \"disc\")\n",
    "discdata.find_features()\n",
    "discdata.remove_redundant_columns()\n",
    "discdata.standardize_all()\n",
    "discdata.compute_agg(True)\n",
    "discdata.clean_covariance()\n",
    "\n",
    "math = pd.read_csv(\"math.csv\")\n",
    "mathdata = agg_dataset()\n",
    "mathdata.load(math, [], [[\"DBN\",\"Grade\"]], \"math\")\n",
    "mathdata.find_features()\n",
    "mathdata.remove_redundant_columns()\n",
    "mathdata.standardize_all()\n",
    "mathdata.compute_agg(True)\n",
    "mathdata.clean_covariance()\n",
    "\n",
    "oss = pd.read_csv(\"oss.csv\")\n",
    "ossdata = agg_dataset()\n",
    "ossdata.load(oss, [], [\"DBN\"], \"oss\")\n",
    "ossdata.find_features()\n",
    "ossdata.remove_redundant_columns()\n",
    "ossdata.standardize_all()\n",
    "ossdata.compute_agg(True)\n",
    "ossdata.clean_covariance()\n",
    "\n",
    "pe = pd.read_csv(\"pe.csv\")\n",
    "pedata = agg_dataset()\n",
    "pedata.load(pe, [], [\"DBN\"], \"pe\")\n",
    "pedata.find_features()\n",
    "pedata.remove_redundant_columns()\n",
    "pedata.standardize_all()\n",
    "pedata.compute_agg(True)\n",
    "pedata.clean_covariance()\n",
    "\n",
    "s2tr = pd.read_csv(\"s2tr.csv\")\n",
    "s2trdata = agg_dataset()\n",
    "s2trdata.load(s2tr, [], [\"DBN\"], \"s2tr\")\n",
    "s2trdata.find_features()\n",
    "s2trdata.remove_redundant_columns()\n",
    "s2trdata.standardize_all()\n",
    "s2trdata.compute_agg(True)\n",
    "s2trdata.clean_covariance()\n",
    "\n",
    "sat = pd.read_csv(\"sat.csv\")\n",
    "satdata = agg_dataset()\n",
    "satdata.load(sat, [], [\"DBN\"], \"sat\")\n",
    "satdata.find_features()\n",
    "satdata.remove_redundant_columns()\n",
    "satdata.standardize_all()\n",
    "satdata.compute_agg(True)\n",
    "satdata.clean_covariance()\n",
    "\n",
    "pro = pd.read_csv(\"Schools_Progress_Report_2012-2013.csv\")\n",
    "prodata = agg_dataset()\n",
    "prodata.load(pro, [], [\"DBN\"], \"pro\")\n",
    "prodata.find_features()\n",
    "prodata.remove_redundant_columns()\n",
    "prodata.standardize_all()\n",
    "prodata.compute_agg(True)\n",
    "prodata.clean_covariance()\n",
    "\n",
    "spy = pd.read_csv(\"spy.csv\")\n",
    "spydata = agg_dataset()\n",
    "spydata.load(spy, [], [\"Year\"], \"spy\")\n",
    "spydata.find_features()\n",
    "spydata.remove_redundant_columns()\n",
    "spydata.standardize_all()\n",
    "spydata.compute_agg(True)\n",
    "spydata.clean_covariance()\n",
    "\n",
    "transfer = pd.read_csv(\"transfer.csv\")\n",
    "transferdata = agg_dataset()\n",
    "transferdata.load(transfer, [], [\"DBN\"], \"transfer\")\n",
    "transferdata.find_features()\n",
    "transferdata.remove_redundant_columns()\n",
    "transferdata.standardize_all()\n",
    "transferdata.compute_agg(True)\n",
    "transferdata.clean_covariance()\n",
    "\n",
    "yabc = pd.read_csv(\"yabc.csv\")\n",
    "yabcdata = agg_dataset()\n",
    "yabcdata.load(yabc, [], [\"DBN\"], \"yabc\")\n",
    "yabcdata.find_features()\n",
    "yabcdata.remove_redundant_columns()\n",
    "yabcdata.standardize_all()\n",
    "yabcdata.compute_agg(True)\n",
    "yabcdata.clean_covariance()\n",
    "\n",
    "dm1 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.22rr-ujq3\")\n",
    "dm1data = agg_dataset()\n",
    "dm1data.load(dm1, [], [\"DBN\"], \"dm1\")\n",
    "dm1data.find_features()\n",
    "dm1data.remove_redundant_columns()\n",
    "dm1data.standardize_all()\n",
    "dm1data.compute_agg(True)\n",
    "dm1data.clean_covariance()\n",
    "\n",
    "dm2 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.25aa-q86c\")\n",
    "dm2data = agg_dataset()\n",
    "dm2data.load(dm2, [], [\"DBN\"], \"dm2\")\n",
    "dm2data.find_features()\n",
    "dm2data.remove_redundant_columns()\n",
    "dm2data.standardize_all()\n",
    "dm2data.compute_agg(True)\n",
    "dm2data.clean_covariance()\n",
    "\n",
    "dm3 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.29bv-qqsy\")\n",
    "dm3data = agg_dataset()\n",
    "dm3data.load(dm3, [], [\"DBN\"], \"dm3\")\n",
    "dm3data.find_features()\n",
    "dm3data.remove_redundant_columns()\n",
    "dm3data.standardize_all()\n",
    "dm3data.compute_agg(True)\n",
    "dm3data.clean_covariance()\n",
    "\n",
    "dm4 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.29ry-u5bf\")\n",
    "dm4data = agg_dataset()\n",
    "dm4data.load(dm4, [], [\"DBN\"], \"dm4\")\n",
    "dm4data.find_features()\n",
    "dm4data.remove_redundant_columns()\n",
    "dm4data.standardize_all()\n",
    "dm4data.compute_agg(True)\n",
    "dm4data.clean_covariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ae02f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccf5cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b734fa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = []\n",
    "for i in range(20):\n",
    "    ran_df = pd.read_csv(\"random_dbn_data.csv\")\n",
    "    ran_df[\"att1\"] = np.random.randint(1, 100, 2000)\n",
    "    ran_df[\"att2\"] = np.random.randint(1, 100, 2000)\n",
    "    ran_df[\"att3\"] = np.random.randint(1, 100, 2000)\n",
    "    ran_df[\"att4\"] = np.random.randint(1, 100, 2000)\n",
    "    ran_df[\"att5\"] = np.random.randint(1, 100, 2000)\n",
    "    randomdata = agg_dataset()\n",
    "    randomdata.load(ran_df, [], [\"DBN\"], \"random\" + str(i))\n",
    "    randomdata.find_features()\n",
    "    randomdata.remove_redundant_columns()\n",
    "    randomdata.standardize_all()\n",
    "    randomdata.compute_agg(True)\n",
    "    randomdata.clean_covariance()\n",
    "    sellers.append((randomdata, \"random\" + str(i)))\n",
    "    \n",
    "sellersdict = dict()\n",
    "\n",
    "for sellerdata, dimension in sellers:\n",
    "    sellersdict[sellerdata.name] = sellerdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c39d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "sellers = [(crimedata, \"DBN\"), (apdata, \"DBN\"), (surveydata, \"DBN\"), \n",
    "           (basedata, \"DBN\"), (discdata, \"DBN\"), \n",
    "           (ossdata, \"DBN\"), (pedata, \"DBN\"), \n",
    "           (s2trdata, \"DBN\"), (satdata, \"DBN\"), (prodata, \"DBN\"),\n",
    "           (transferdata, \"DBN\"), (yabcdata, \"DBN\"), (dm1data, \"DBN\"),\n",
    "           (dm2data, \"DBN\"), (dm3data, \"DBN\"), (dm4data, \"DBN\")]\n",
    "\n",
    "sellersdict = dict()\n",
    "\n",
    "for sellerdata, dimension in sellers:\n",
    "    sellersdict[sellerdata.name] = sellerdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d3507c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0ae1998f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random0\n",
      "random1\n",
      "random2\n",
      "random3\n",
      "random4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/arda/arda-datasets/school/datamarket.py:350: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  join.drop('cov:c_y', 1, inplace=True)\n",
      "/home/jupyter/arda/arda-datasets/school/datamarket.py:383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  join['cov:Q:' + att2 + \",\" + att1] = join['cov:s:' + att2] * join['cov:s:' + att1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random5\n",
      "random6\n",
      "random7\n",
      "random8\n",
      "random9\n",
      "random10\n",
      "random11\n",
      "random12\n",
      "random13\n",
      "random14\n",
      "random15\n",
      "random16\n",
      "random17\n",
      "random18\n",
      "random19\n",
      "CPU times: user 6.01 s, sys: 102 ms, total: 6.11 s\n",
      "Wall time: 5.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# care about left/right ojoin\n",
    "# notice that left table is not missing value imputated\n",
    "# this is O(n m^2)\n",
    "dim_idx = index(\"DBN\")\n",
    "for sellerdata, dimension in sellers:\n",
    "    print(sellerdata.name)\n",
    "    dim_idx.absorb(sellerdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d9399b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 selected out of 86\n",
      "CPU times: user 56 ms, sys: 11.7 ms, total: 67.7 ms\n",
      "Wall time: 50.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim_idx.compute_eigen_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f759459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 704 ms, sys: 32 ms, total: 736 ms\n",
      "Wall time: 733 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim_idx.compute_pc_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485bdcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 ms, sys: 0 ns, total: 1.88 ms\n",
      "Wall time: 1.89 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dim_idx.compute_seller_contribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0c9f1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read seller dataset and split into train/test\n",
    "gender = pd.read_csv(\"gender.csv\")\n",
    "\n",
    "# train test split\n",
    "msk = split_mask(len(gender)) < 0.8\n",
    "gender_train = gender[msk].copy()\n",
    "gender_test = gender[~msk].copy()\n",
    "\n",
    "gender_train_data = agg_dataset()\n",
    "gender_train_data.load(gender_train, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_train_data.process_target(\"Mean Scale Score\")\n",
    "gender_train_data.to_numeric_and_impute_all()\n",
    "gender_train_data.remove_redundant_columns()\n",
    "gender_train_data.compute_agg()\n",
    "\n",
    "gender_test_data = agg_dataset()\n",
    "gender_test_data.load(gender_test, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_test_data.process_target(\"Mean Scale Score\")\n",
    "gender_test_data.to_numeric_and_impute_all()\n",
    "gender_test_data.remove_redundant_columns()\n",
    "gender_test_data.compute_agg()\n",
    "\n",
    "y = \"Mean Scale Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "42d7b047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'f9'] 0.45914810498980463\n",
      "CPU times: user 206 ms, sys: 4.03 ms, total: 210 ms\n",
      "Wall time: 208 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:350: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cur_atts, final_r2 = select_features(gender_train_data, gender_test_data, dim_idx, \"DBN\", 10, y, pca=True)\n",
    "print(cur_atts, final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "8f9225f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pro:2012-2013 PERFORMANCE CATEGORY SCORE',\n",
       " 'dm2:MATH %Level 1',\n",
       " 'pro:2012-2013 ADDITIONAL CREDIT',\n",
       " 'survey:Total Safety and Respect Score',\n",
       " 'gender:Mean Scale Score']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_train_data.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef21a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "71a5bd1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "917036ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:350: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "survey ['survey:Total Safety and Respect Score', 'survey:Total Engagement Score', 'gender:Number Tested', 'survey:Total Academic Expectations Score', 'survey:Total Communication Score', 'survey:Total Teacher Response Rate (%)'] 0.2455648119511481\n",
      "3\n",
      "pro ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'survey:Total Engagement Score', 'pro:DISTRICT', 'survey:Total Communication Score', 'pro:2012-13 OVERALL PERCENTILE', 'pro:2012-2013 OVERALL SCORE', 'survey:Total Teacher Response Rate (%)', 'gender:Number Tested'] 0.4502965331693365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "dm2 ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'dm2:#Female', 'dm2:ELA #Test Takers', 'dm2:MATH %Level 2'] 0.5254591842021068\n",
      "9\n",
      "sat ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'dm2:#Female', 'dm2:ELA #Test Takers', 'dm2:MATH %Level 2'] 0.5254591842021068\n",
      "9\n",
      "pe ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'pe:Student Enrollment', 'dm2:#Female', 'dm2:MATH %Level 2'] 0.5395518368262413\n",
      "9\n",
      "dm4 ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'pe:Student Enrollment', 'dm2:#Female', 'dm2:MATH %Level 2'] 0.5395518368262413\n",
      "CPU times: user 4.02 s, sys: 76.5 ms, total: 4.1 s\n",
      "Wall time: 4.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(6):\n",
    "    cur_atts, final_r2 = select_features(gender_train_data, gender_test_data, dim_idx, \"DBN\", 10, y, pca=True)\n",
    "    if cur_atts[-1].startswith(\"f\") and cur_atts[-1][1:].isnumeric():\n",
    "        # idx is the best pc\n",
    "        idx = int(cur_atts[-1][1:])\n",
    "        print(idx)\n",
    "        # find the seller dataset contributes to it most\n",
    "        for i in np.argsort(dim_idx.datasets_weights[idx])[::-1]:\n",
    "            sellername = dim_idx.datasets[i]\n",
    "            if sellername in gender_train_data.datasets:\n",
    "                continue\n",
    "\n",
    "            cur_atts2, final_r22 = select_features(gender_train_data, gender_test_data, sellersdict[sellername], \"DBN\",10,y)\n",
    "            print(sellername, cur_atts2, final_r22)\n",
    "            if len([x for x in cur_atts2 if x in sellersdict[sellername].X]) == 0:\n",
    "                # if not good, just add it to list, and exclude it\n",
    "                gender_train_data.datasets.add(sellername)\n",
    "                gender_test_data.datasets.add(sellername)\n",
    "            else:\n",
    "                gender_train_data.absorb(sellersdict[sellername], \"DBN\", cur_atts2 + [gender_train_data.name + \":\" + y])\n",
    "                gender_test_data.absorb(sellersdict[sellername], \"DBN\", cur_atts2 + [gender_train_data.name + \":\" + y])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa721a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1         15\n",
       "4          7\n",
       "5          9\n",
       "9         14\n",
       "10        12\n",
       "        ... \n",
       "47729    148\n",
       "47730    161\n",
       "47731    125\n",
       "47732    141\n",
       "47734    144\n",
       "Name: cov:s:gender:Number Tested, Length: 37981, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_train_data.data['cov:s:gender:Number Tested']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "da70cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read seller dataset and split into train/test\n",
    "gender = pd.read_csv(\"gender.csv\")\n",
    "\n",
    "# train test split\n",
    "gender_train = gender[msk].copy()\n",
    "gender_test = gender[~msk].copy()\n",
    "\n",
    "gender_train_data = agg_dataset()\n",
    "gender_train_data.load(gender_train, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_train_data.process_target(\"Mean Scale Score\")\n",
    "gender_train_data.to_numeric_and_impute_all()\n",
    "gender_train_data.remove_redundant_columns()\n",
    "gender_train_data.compute_agg()\n",
    "\n",
    "gender_test_data = agg_dataset()\n",
    "gender_test_data.load(gender_test, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_test_data.process_target(\"Mean Scale Score\")\n",
    "gender_test_data.to_numeric_and_impute_all()\n",
    "gender_test_data.remove_redundant_columns()\n",
    "gender_test_data.compute_agg()\n",
    "\n",
    "y = \"Mean Scale Score\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9154b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2924c0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:350: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sat\n",
      "pro\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "pro ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'pro:2012-2013 ADDITIONAL CREDIT', 'pro:DISTRICT', 'pro:2012-2013 ENVIRONMENT CATEGORY SCORE', 'pro:2012-13 OVERALL PERCENTILE', 'pro:2012-2013 PROGRESS CATEGORY SCORE', 'pro:2012-2013 OVERALL SCORE'] 0.3891125851994053\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:383: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dm3\n",
      "dm4\n",
      "dm2 ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'dm2:#Hispanic', 'dm2:%Hispanic', 'dm2:Total Enrollment', 'dm2:#Male', 'dm2:MATH %Level 2', 'dm2:MATH #Level 2', 'dm2:ELA #Level 1'] 0.4855904335290271\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm3\n",
      "dm4\n",
      "survey ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'dm2:#Male', 'dm2:MATH %Level 2', 'dm2:MATH #Level 2'] 0.5251899818665657\n",
      "crime\n",
      "ap\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm3\n",
      "dm4\n",
      "pe ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'pe:Student Enrollment', 'dm2:#Male', 'dm2:MATH %Level 2'] 0.5365322583036518\n",
      "crime\n",
      "ap\n",
      "base\n",
      "disc\n",
      "oss\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm3\n",
      "dm4\n",
      "dm4 ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'pe:Student Enrollment', 'dm2:#Male', 'dm4:ELA #Level 1'] 0.5369036167885715\n",
      "crime\n",
      "ap\n",
      "base\n",
      "disc\n",
      "oss\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm3\n",
      "crime ['pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'dm2:MATH %Level 1', 'pro:2012-2013 ADDITIONAL CREDIT', 'survey:Total Safety and Respect Score', 'dm2:%Hispanic', 'survey:Total Engagement Score', 'dm2:Total Enrollment', 'pe:Student Enrollment', 'dm2:#Male', 'dm4:ELA #Level 1'] 0.5369036167885715\n",
      "CPU times: user 12.2 s, sys: 211 ms, total: 12.4 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for _ in range(6):\n",
    "    best_seller = None\n",
    "    best_seller_attrs = []\n",
    "    best_dimension = None\n",
    "    best_r2 = 0\n",
    "    \n",
    "    for sellerdata, dimension in sellers:\n",
    "        # check if current seller has been bought\n",
    "        if sellerdata.name in gender_train_data.datasets:\n",
    "            continue\n",
    "        print(sellerdata.name)\n",
    "        \n",
    "         # find the attributes and r2 of augmenting\n",
    "        cur_atts, final_r2 = select_features(gender_train_data, gender_test_data, sellerdata, dimension, 10, y)\n",
    "        \n",
    "        if final_r2 > best_r2:\n",
    "            best_seller = sellerdata\n",
    "            best_dimension = dimension\n",
    "            best_seller_attrs = cur_atts\n",
    "            best_r2 = final_r2\n",
    "    \n",
    "    print(best_seller.name, best_seller_attrs, best_r2)\n",
    "    \n",
    "    if len([x for x in best_seller_attrs if x in best_seller.X]) == 0:\n",
    "        gender_train_data.datasets.add(best_seller)\n",
    "        gender_test_data.datasets.add(best_seller)\n",
    "    else:\n",
    "        gender_train_data.absorb(best_seller, \"DBN\", best_seller_attrs + [gender_train_data.name + \":\" + y])\n",
    "        gender_test_data.absorb(best_seller, \"DBN\", best_seller_attrs + [gender_train_data.name + \":\" + y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcb74b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fde15fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00a753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
