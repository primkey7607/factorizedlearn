{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "afb4460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamarket import *\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0687ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fb5e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read gender from \n",
    "gender = pd.read_csv(\"gender.csv\")\n",
    "\n",
    "# train test split\n",
    "msk = split_mask(len(gender)) < 0.8\n",
    "gender_train = gender[msk].copy()\n",
    "gender_test = gender[~msk].copy()\n",
    "\n",
    "gender_train_data = agg_dataset()\n",
    "gender_train_data.load(gender_train, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_train_data.process_target(\"Mean Scale Score\")\n",
    "gender_train_data.to_numeric_and_impute_all()\n",
    "gender_train_data.remove_redundant_columns()\n",
    "gender_train_data.create_count_true()\n",
    "gender_train_data.compute_agg()\n",
    "\n",
    "gender_test_data = agg_dataset()\n",
    "gender_test_data.load(gender_test, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_test_data.process_target(\"Mean Scale Score\")\n",
    "gender_test_data.to_numeric_and_impute_all()\n",
    "gender_test_data.remove_redundant_columns()\n",
    "gender_test_data.create_count_true()\n",
    "gender_test_data.compute_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36a1916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive_join = pd.merge(gender_train[[\"DBN\", \"Mean Scale Score\"]].set_index(\"DBN\"), crime[[\"DBN\", \"Register\"]].set_index(\"DBN\"), how=\"left\", left_index=True, right_index=True)\n",
    "# naive_join[\"Register\"].fillna(value=crimedata.covariance[\"cov:s:crime:Register\"], inplace=True)\n",
    "# naive_join[\"Register\"].corr(naive_join[\"Mean Scale Score\"])\n",
    "# (naive_join[\"Register\"] * naive_join[\"Register\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e3910780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/arda/arda-datasets/school/datamarket.py:201: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.data['cov:Q:' + tablename + \":\" + attributes[i] + \",\"+ tablename + \":\" + attributes[j]] = self.data[attributes[i]] * self.data[attributes[j]]\n"
     ]
    }
   ],
   "source": [
    "# read buyer dataste\n",
    "crime = pd.read_csv(\"crime.csv\")\n",
    "crimedata = agg_dataset()\n",
    "crimedata.load(crime, [], [\"DBN\"], \"crime\")\n",
    "crimedata.find_features()\n",
    "crimedata.remove_redundant_columns()\n",
    "crimedata.create_count_true()\n",
    "crimedata.compute_agg(True)\n",
    "\n",
    "\n",
    "esl = pd.read_csv(\"esl.csv\")\n",
    "esldata = agg_dataset()\n",
    "esldata.load(esl, [], [[\"DBN\",\"Grade\"]], \"esl\")\n",
    "esldata.find_features()\n",
    "esldata.remove_redundant_columns()\n",
    "esldata.create_count_true()\n",
    "esldata.compute_agg(True)\n",
    "\n",
    "ap = pd.read_csv(\"ap.csv\")\n",
    "apdata = agg_dataset()\n",
    "apdata.load(ap, [], [\"DBN\"], \"ap\")\n",
    "apdata.find_features()\n",
    "apdata.remove_redundant_columns()\n",
    "apdata.create_count_true()\n",
    "apdata.compute_agg(True)\n",
    "\n",
    "survey = pd.read_csv(\"2013_NYC_School_Survey.csv\")\n",
    "surveydata = agg_dataset()\n",
    "surveydata.load(survey, [], [\"DBN\"], \"survey\")\n",
    "surveydata.find_features()\n",
    "surveydata.remove_redundant_columns()\n",
    "surveydata.create_count_true()\n",
    "surveydata.compute_agg(True)\n",
    "\n",
    "base = pd.read_csv(\"base.csv\")\n",
    "basedata = agg_dataset()\n",
    "basedata.load(base, [], [\"DBN\"], \"base\")\n",
    "basedata.find_features()\n",
    "basedata.remove_redundant_columns()\n",
    "basedata.create_count_true()\n",
    "basedata.compute_agg(True)\n",
    "\n",
    "disc = pd.read_csv(\"disc.csv\")\n",
    "discdata = agg_dataset()\n",
    "discdata.load(disc, [], [\"DBN\"], \"disc\")\n",
    "discdata.find_features()\n",
    "discdata.remove_redundant_columns()\n",
    "discdata.create_count_true()\n",
    "discdata.compute_agg(True)\n",
    "\n",
    "math = pd.read_csv(\"math.csv\")\n",
    "mathdata = agg_dataset()\n",
    "mathdata.load(math, [], [[\"DBN\",\"Grade\"]], \"math\")\n",
    "mathdata.find_features()\n",
    "mathdata.remove_redundant_columns()\n",
    "mathdata.create_count_true()\n",
    "mathdata.compute_agg(True)\n",
    "\n",
    "oss = pd.read_csv(\"oss.csv\")\n",
    "ossdata = agg_dataset()\n",
    "ossdata.load(oss, [], [\"DBN\"], \"oss\")\n",
    "ossdata.find_features()\n",
    "ossdata.remove_redundant_columns()\n",
    "ossdata.create_count_true()\n",
    "ossdata.compute_agg(True)\n",
    "\n",
    "pe = pd.read_csv(\"pe.csv\")\n",
    "pedata = agg_dataset()\n",
    "pedata.load(pe, [], [\"DBN\"], \"pe\")\n",
    "pedata.find_features()\n",
    "pedata.remove_redundant_columns()\n",
    "pedata.create_count_true()\n",
    "pedata.compute_agg(True)\n",
    "\n",
    "s2tr = pd.read_csv(\"s2tr.csv\")\n",
    "s2trdata = agg_dataset()\n",
    "s2trdata.load(s2tr, [], [\"DBN\"], \"s2tr\")\n",
    "s2trdata.find_features()\n",
    "s2trdata.remove_redundant_columns()\n",
    "s2trdata.create_count_true()\n",
    "s2trdata.compute_agg(True)\n",
    "\n",
    "sat = pd.read_csv(\"sat.csv\")\n",
    "satdata = agg_dataset()\n",
    "satdata.load(sat, [], [\"DBN\"], \"sat\")\n",
    "satdata.find_features()\n",
    "satdata.remove_redundant_columns()\n",
    "satdata.create_count_true()\n",
    "satdata.compute_agg(True)\n",
    "\n",
    "pro = pd.read_csv(\"Schools_Progress_Report_2012-2013.csv\")\n",
    "prodata = agg_dataset()\n",
    "prodata.load(pro, [], [\"DBN\"], \"pro\")\n",
    "prodata.find_features()\n",
    "prodata.remove_redundant_columns()\n",
    "prodata.create_count_true()\n",
    "prodata.compute_agg(True)\n",
    "\n",
    "\n",
    "spy = pd.read_csv(\"spy.csv\")\n",
    "spydata = agg_dataset()\n",
    "spydata.load(spy, [], [\"Year\"], \"spy\")\n",
    "spydata.find_features()\n",
    "spydata.remove_redundant_columns()\n",
    "spydata.create_count_true()\n",
    "spydata.compute_agg(True)\n",
    "\n",
    "transfer = pd.read_csv(\"transfer.csv\")\n",
    "transferdata = agg_dataset()\n",
    "transferdata.load(transfer, [], [\"DBN\"], \"transfer\")\n",
    "transferdata.find_features()\n",
    "transferdata.remove_redundant_columns()\n",
    "transferdata.create_count_true()\n",
    "transferdata.compute_agg(True)\n",
    "\n",
    "yabc = pd.read_csv(\"yabc.csv\")\n",
    "yabcdata = agg_dataset()\n",
    "yabcdata.load(yabc, [], [\"DBN\"], \"yabc\")\n",
    "yabcdata.find_features()\n",
    "yabcdata.remove_redundant_columns()\n",
    "yabcdata.create_count_true()\n",
    "yabcdata.compute_agg(True)\n",
    "\n",
    "dm1 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.22rr-ujq3\")\n",
    "dm1data = agg_dataset()\n",
    "dm1data.load(dm1, [], [\"DBN\"], \"dm1\")\n",
    "dm1data.find_features()\n",
    "dm1data.remove_redundant_columns()\n",
    "dm1data.create_count_true()\n",
    "dm1data.compute_agg(True)\n",
    "\n",
    "dm2 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.25aa-q86c\")\n",
    "dm2data = agg_dataset()\n",
    "dm2data.load(dm2, [], [\"DBN\"], \"dm2\")\n",
    "dm2data.find_features()\n",
    "dm2data.remove_redundant_columns()\n",
    "dm2data.create_count_true()\n",
    "dm2data.compute_agg(True)\n",
    "\n",
    "dm3 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.29bv-qqsy\")\n",
    "dm3data = agg_dataset()\n",
    "dm3data.load(dm3, [], [\"DBN\"], \"dm3\")\n",
    "dm3data.find_features()\n",
    "dm3data.remove_redundant_columns()\n",
    "dm3data.create_count_true()\n",
    "dm3data.compute_agg(True)\n",
    "\n",
    "dm4 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.29ry-u5bf\")\n",
    "dm4data = agg_dataset()\n",
    "dm4data.load(dm4, [], [\"DBN\"], \"dm4\")\n",
    "dm4data.find_features()\n",
    "dm4data.remove_redundant_columns()\n",
    "dm4data.create_count_true()\n",
    "dm4data.compute_agg(True)\n",
    "\n",
    "dm5 = pd.read_csv(\"other/datamart.socrata.data-cityofnewyork-us.43qc-8vv8\")\n",
    "dm5data = agg_dataset()\n",
    "dm5data.load(dm5, [], [[\"DBN\",\"Grade\"]], \"dm5\")\n",
    "dm5data.find_features()\n",
    "dm5data.remove_redundant_columns()\n",
    "dm5data.create_count_true()\n",
    "dm5data.compute_agg(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d7d52794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "math\n",
      "oss\n",
      "pe\n",
      "s2tr\n",
      "sat\n",
      "pro\n",
      "spy\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "dm5\n",
      "math ['math:Level 4 %', 'math:Level 1 %', 'math:Year', 'math:Mean Scale Score'] 0.3910558224750671\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "s2tr\n",
      "sat\n",
      "pro\n",
      "spy\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "dm5\n",
      "pro ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'pro:2012-2013 ADDITIONAL CREDIT', 'math:Level 1 %'] 0.49240436408192334\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "s2tr\n",
      "sat\n",
      "spy\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "dm5\n",
      "spy ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'spy:Year Close ', 'pro:2012-2013 ADDITIONAL CREDIT'] 0.5249417169683165\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "s2tr\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "dm5\n",
      "s2tr ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 's2tr:School Pupil-Teacher Ratio', 'spy:Year Close '] 0.544448046994143\n",
      "crime\n",
      "ap\n",
      "survey\n",
      "base\n",
      "disc\n",
      "oss\n",
      "pe\n",
      "sat\n",
      "transfer\n",
      "yabc\n",
      "dm1\n",
      "dm2\n",
      "dm3\n",
      "dm4\n",
      "dm5\n",
      "dm5 ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 's2tr:School Pupil-Teacher Ratio', 'spy:Year Close '] 0.544448046994197\n",
      "CPU times: user 9.3 s, sys: 58.7 ms, total: 9.36 s\n",
      "Wall time: 9.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sellers = [(crimedata, \"DBN\"), (apdata, \"DBN\"), (surveydata, \"DBN\"), \n",
    "           (basedata, \"DBN\"), (discdata, \"DBN\"), (mathdata, (\"DBN\",\"Grade\")), \n",
    "           (ossdata, \"DBN\"), (pedata, \"DBN\"), (s2trdata, \"DBN\"), \n",
    "           (satdata, \"DBN\"), (prodata, \"DBN\"), (spydata, \"Year\"),\n",
    "           (transferdata, \"DBN\"), (yabcdata, \"DBN\"), (dm1data, \"DBN\"),\n",
    "           (dm2data, \"DBN\"), (dm3data, \"DBN\"), (dm4data, \"DBN\"), (dm5data, (\"DBN\",\"Grade\"))]\n",
    "\n",
    "# sellers = [(mathdata, (\"DBN\",\"Grade\"))]\n",
    "\n",
    "# find m best datasets to augment\n",
    "m = 5\n",
    "y = \"Mean Scale Score\"\n",
    "\n",
    "for i in range(m):\n",
    "    best_seller = None\n",
    "    best_seller_attrs = []\n",
    "    best_dimension = None\n",
    "    best_r2 = 0\n",
    "\n",
    "    for sellerdata, dimension in sellers:\n",
    "        # check if current seller has been bought\n",
    "        if sellerdata.name in gender_train_data.datasets:\n",
    "            continue\n",
    "        print(sellerdata.name)\n",
    "\n",
    "        # find the attributes and r2 of augmenting\n",
    "        cur_atts, final_r2 = select_features(gender_train_data, gender_test_data, sellerdata, dimension, 4, y)\n",
    "\n",
    "        if final_r2 > best_r2:\n",
    "            best_seller = sellerdata\n",
    "            best_dimension = dimension\n",
    "            best_seller_attrs = cur_atts\n",
    "            best_r2 = final_r2\n",
    "\n",
    "\n",
    "    print(best_seller.name, best_seller_attrs, best_r2)\n",
    "    \n",
    "    true_absorb = True\n",
    "    budget = 300000000\n",
    "    \n",
    "    # there is no attributes that is predictive in seller dataset\n",
    "    if len([x for x in best_seller_attrs if x in best_seller.X]) == 0:\n",
    "        true_absorb = False\n",
    "    else:\n",
    "        count_true_join = connect_count(gender_train_data, best_seller, best_dimension)\n",
    "        if count_true_join[\"cov:c\"].sum() > budget:\n",
    "            print(\"beyond budget!!\")\n",
    "            true_absorb = False\n",
    "        else:\n",
    "            gender_train_data.count_true = count_true_join\n",
    "            gender_train_data.count_true.reset_index(inplace=True)\n",
    "    \n",
    "    if not true_absorb:\n",
    "        gender_train_data.datasets.add(best_seller.name)\n",
    "        gender_test_data.datasets.add(best_seller.name)\n",
    "    else:\n",
    "        gender_train_data.absorb(best_seller, best_dimension, best_seller_attrs + [gender_train_data.name + \":\" + y])\n",
    "        gender_test_data.absorb(best_seller, best_dimension, best_seller_attrs + [gender_train_data.name + \":\" + y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13fc742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "57046d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_train_data = agg_dataset()\n",
    "gender_train_data.load(gender_train, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_train_data.process_target(\"Mean Scale Score\")\n",
    "gender_train_data.to_numeric_and_impute_all()\n",
    "gender_train_data.remove_redundant_columns()\n",
    "gender_train_data.create_count_true()\n",
    "gender_train_data.compute_agg()\n",
    "\n",
    "gender_test_data = agg_dataset()\n",
    "gender_test_data.load(gender_test, [\"Number Tested\", \"Mean Scale Score\"], [\"DBN\", [\"DBN\",\"Grade\"], \"Year\", \"Category\"], \"gender\")\n",
    "gender_test_data.process_target(\"Mean Scale Score\")\n",
    "gender_test_data.to_numeric_and_impute_all()\n",
    "gender_test_data.remove_redundant_columns()\n",
    "gender_test_data.create_count_true()\n",
    "gender_test_data.compute_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f5749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d43db555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crime\n",
      "0.2603666902587522\n",
      "ap\n",
      "0.2603666902587522\n",
      "survey\n",
      "0.41696893969149107\n",
      "base\n",
      "0.41696893969149107\n",
      "disc\n",
      "0.41696893969149107\n",
      "math\n",
      "0.6338383601758336\n",
      "oss\n",
      "0.6338383601758336\n",
      "pe\n",
      "0.6338383601758336\n",
      "s2tr\n",
      "0.6338383601758336\n",
      "sat\n",
      "0.6338383601758336\n",
      "pro\n",
      "0.6338383601758336\n",
      "spy\n",
      "0.6338383601758336\n",
      "transfer\n",
      "yabc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6338383601758336\n",
      "dm1\n",
      "0.6338383601758336\n",
      "dm2\n",
      "0.6338383601758336\n",
      "dm3\n",
      "0.6338383601758336\n",
      "dm4\n",
      "0.6338383601758336\n",
      "dm5\n",
      "math ['math:Level 4 %', 'math:Level 1 %', 'math:Year', 'math:Mean Scale Score'] 0.3910558224750671\n",
      "crime\n",
      "0.2603666902587522\n",
      "ap\n",
      "0.2603666902587522\n",
      "survey\n",
      "0.41696893969149107\n",
      "base\n",
      "0.41696893969149107\n",
      "disc\n",
      "0.41696893969149107\n",
      "oss\n",
      "0.41696893969149107\n",
      "pe\n",
      "0.41696893969149107\n",
      "s2tr\n",
      "0.4602226404661219\n",
      "sat\n",
      "0.4602226404661219\n",
      "pro\n",
      "0.5546031186552208\n",
      "spy\n",
      "0.5546031186552208\n",
      "transfer\n",
      "yabc\n",
      "0.5546031186552208\n",
      "dm1\n",
      "0.5546031186552208\n",
      "dm2\n",
      "0.5546031186552208\n",
      "dm3\n",
      "0.5546031186552208\n",
      "dm4\n",
      "0.5546031186552208\n",
      "dm5\n",
      "pro ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 'pro:2012-2013 ADDITIONAL CREDIT', 'math:Level 1 %'] 0.49240436408192334\n",
      "crime\n",
      "0.2603666902587522\n",
      "ap\n",
      "0.2603666902587522\n",
      "survey\n",
      "0.41696893969149107\n",
      "base\n",
      "0.41696893969149107\n",
      "disc\n",
      "0.41696893969149107\n",
      "oss\n",
      "0.41696893969149107\n",
      "pe\n",
      "0.41696893969149107\n",
      "s2tr\n",
      "0.4602226404661219\n",
      "sat\n",
      "0.4602226404661219\n",
      "spy\n",
      "0.4602226404661219\n",
      "transfer\n",
      "yabc\n",
      "0.4602226404661219\n",
      "dm1\n",
      "0.4602226404661219\n",
      "dm2\n",
      "0.4602226404661219\n",
      "dm3\n",
      "0.4602226404661219\n",
      "dm4\n",
      "0.4602226404661219\n",
      "dm5\n",
      "s2tr ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 's2tr:School Pupil-Teacher Ratio', 'pro:2012-2013 ADDITIONAL CREDIT'] 0.51963742063518\n",
      "crime\n",
      "0.2603666902587522\n",
      "ap\n",
      "0.2603666902587522\n",
      "survey\n",
      "0.41696893969149107\n",
      "base\n",
      "0.41696893969149107\n",
      "disc\n",
      "0.41696893969149107\n",
      "oss\n",
      "0.41696893969149107\n",
      "pe\n",
      "0.41696893969149107\n",
      "sat\n",
      "0.41696893969149107\n",
      "spy\n",
      "0.41696893969149107\n",
      "transfer\n",
      "yabc\n",
      "0.41696893969149107\n",
      "dm1\n",
      "0.41696893969149107\n",
      "dm2\n",
      "0.41696893969149107\n",
      "dm3\n",
      "0.41696893969149107\n",
      "dm4\n",
      "0.41696893969149107\n",
      "dm5\n",
      "survey ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 's2tr:School Pupil-Teacher Ratio', 'survey:Total Safety and Respect Score'] 0.5295858982477584\n",
      "crime\n",
      "0.2603666902587522\n",
      "ap\n",
      "0.2603666902587522\n",
      "base\n",
      "0.2603666902587522\n",
      "disc\n",
      "0.2603666902587522\n",
      "oss\n",
      "0.2603666902587522\n",
      "pe\n",
      "0.28438839243058883\n",
      "sat\n",
      "0.28438839243058883\n",
      "spy\n",
      "0.28438839243058883\n",
      "transfer\n",
      "yabc\n",
      "0.28438839243058883\n",
      "dm1\n",
      "0.28438839243058883\n",
      "dm2\n",
      "0.28438839243058883\n",
      "dm3\n",
      "0.28438839243058883\n",
      "dm4\n",
      "0.28438839243058883\n",
      "dm5\n",
      "pe ['math:Level 4 %', 'pro:2012-2013 PERFORMANCE CATEGORY SCORE', 's2tr:School Pupil-Teacher Ratio', 'survey:Total Safety and Respect Score'] 0.5295858982477584\n",
      "CPU times: user 2.76 s, sys: 60.4 ms, total: 2.82 s\n",
      "Wall time: 2.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sellers = [(crimedata, \"DBN\"), (apdata, \"DBN\"), (surveydata, \"DBN\"), \n",
    "           (basedata, \"DBN\"), (discdata, \"DBN\"), (mathdata, (\"DBN\",\"Grade\")), \n",
    "           (ossdata, \"DBN\"), (pedata, \"DBN\"), (s2trdata, \"DBN\"), \n",
    "           (satdata, \"DBN\"), (prodata, \"DBN\"), (spydata, \"Year\"),\n",
    "           (transferdata, \"DBN\"), (yabcdata, \"DBN\"), (dm1data, \"DBN\"),\n",
    "           (dm2data, \"DBN\"), (dm3data, \"DBN\"), (dm4data, \"DBN\"), (dm5data, (\"DBN\",\"Grade\"))]\n",
    "\n",
    "# sellers = [(mathdata, (\"DBN\",\"Grade\"))]\n",
    "\n",
    "# find m best datasets to augment\n",
    "m = 5\n",
    "\n",
    "\n",
    "for i in range(m):\n",
    "    best_seller = None\n",
    "    best_seller_attrs = []\n",
    "    best_dimension = None\n",
    "    best_r2 = 0\n",
    "    y = \"gender:Mean Scale Score\"\n",
    "    \n",
    "    for sellerdata, dimension in sellers:\n",
    "        # check if current seller has been bought\n",
    "        if sellerdata.name in gender_train_data.datasets:\n",
    "            continue\n",
    "        print(sellerdata.name)\n",
    "        \n",
    "        try:\n",
    "            join_data = connect_correlation(gender_train_data, sellerdata, dimension, y)\n",
    "            join_cov = join_data.sum()\n",
    "\n",
    "            for x in sellerdata.X:\n",
    "                # here r2 is correlation... bad naming fix later\n",
    "                final_r2 = correlation(join_cov, y, x)\n",
    "                # sometimes it could be inf if variance =0\n",
    "                if final_r2 > best_r2 and final_r2 < 1:\n",
    "                    best_seller = sellerdata\n",
    "                    best_r2 = final_r2\n",
    "                    best_dimension = dimension\n",
    "            print(best_r2)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    y = \"Mean Scale Score\"\n",
    "    # find the attributes and r2 of augmenting\n",
    "    cur_atts, final_r2 = select_features(gender_train_data, gender_test_data, best_seller, best_dimension, 4, y)\n",
    "    best_seller_attrs = cur_atts    \n",
    "   \n",
    "    print(best_seller.name, best_seller_attrs, final_r2)\n",
    "    \n",
    "    true_absorb = True\n",
    "    budget = 300000000\n",
    "    \n",
    "    # there is no attributes that is predictive in seller dataset\n",
    "    if len([x for x in best_seller_attrs if x in best_seller.X]) == 0:\n",
    "        true_absorb = False\n",
    "    else:\n",
    "        count_true_join = connect_count(gender_train_data, best_seller, best_dimension)\n",
    "        if count_true_join[\"cov:c\"].sum() > budget:\n",
    "            print(\"beyond budget!!\")\n",
    "            true_absorb = False\n",
    "        else:\n",
    "            gender_train_data.count_true = count_true_join\n",
    "            gender_train_data.count_true.reset_index(inplace=True)\n",
    "    \n",
    "    if not true_absorb:\n",
    "        gender_train_data.datasets.add(best_seller.name)\n",
    "        gender_test_data.datasets.add(best_seller.name)\n",
    "    else:\n",
    "        gender_train_data.absorb(best_seller, best_dimension, best_seller_attrs + [gender_train_data.name + \":\" + y])\n",
    "        gender_test_data.absorb(best_seller, best_dimension, best_seller_attrs + [gender_train_data.name + \":\" + y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b8ac2c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "join_data = connect_correlation(gender_train_data, mathdata, (\"DBN\",\"Grade\"), 'gender:Mean Scale Score')\n",
    "join_cov = join_data.sum()\n",
    "\n",
    "y = 'gender:Mean Scale Score'\n",
    "\n",
    "for x in mathdata.X:\n",
    "    print(x, correlation(join_cov, y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66064d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d9575b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "28af3119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math:Year -0.01950183008527478\n",
      "math:Number Tested 0.018346291429264593\n",
      "math:Mean Scale Score 0.5373878621222868\n",
      "math:Level 1 # -0.23981943977547446\n",
      "math:Level 1 % -0.4520984683028725\n",
      "math:Level 2 # -0.20748039955333836\n",
      "math:Level 2 % -0.4998006219451567\n",
      "math:Level 3 # 0.024970514217959542\n",
      "math:Level 3 % 0.030186041599590697\n",
      "math:Level 4 # 0.3279983910385263\n",
      "math:Level 4 % 0.6338383601758336\n",
      "math:Level 3+4 # 0.15878916772419865\n",
      "math:Level 3+4 % 0.5294526982719328\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f622dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(join_cov, att1, att2):\n",
    "    count = join_cov['cov:c']\n",
    "    sum_att1 = join_cov['cov:s:' + att1]\n",
    "    sum_att1_sqmean = sum_att1*sum_att1\n",
    "    sum_att2 = join_cov['cov:s:' + att2]\n",
    "    sum_att2_sqmean = sum_att2*sum_att2\n",
    "    sum_att1_att2_sqmean = sum_att1*sum_att2\n",
    "    sum_att1_att1 = join_cov[\"cov:Q:\" + att1 + \",\" + att1]\n",
    "    sum_att2_att2 = join_cov[\"cov:Q:\" + att2 + \",\" + att2]\n",
    "    if \"cov:Q:\" + att1 + \",\" + att2 in join_cov:\n",
    "        sum_att1_att2 = join_cov[\"cov:Q:\" + att1 + \",\" + att2]\n",
    "    else:\n",
    "        sum_att1_att2 = join_cov[\"cov:Q:\" + att2 + \",\" + att1]\n",
    "    return (count * sum_att1_att2 - sum_att1_att2_sqmean)/sqrt((count * sum_att1_att1 - sum_att1_sqmean)*(count * sum_att2_att2 - sum_att2_sqmean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1ab9290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_inp is whether we join of index of aggdata1 (index has good performance. however no index during absorption)\n",
    "# specify right_attrs if for right table, only part of attributes are involved\n",
    "# average specify whether we use mean for missing value imputation\n",
    "# return a pandas dataframe of join result\n",
    "def connect_correlation(aggdata1, aggdata2, dimension, y, left_inp = False,average=True, how = 'left'):\n",
    "    if isinstance(dimension, list):\n",
    "        dimension = tuple(dimension)\n",
    "    \n",
    "    if left_inp:\n",
    "        agg1 = aggdata1.data\n",
    "    else:\n",
    "        agg1 = aggdata1.agg_dimensions[dimension]        \n",
    "        \n",
    "    agg2 = aggdata2.agg_dimensions[dimension]\n",
    "    \n",
    "    left_attributes = [y]\n",
    "    left_tablename = aggdata1.name\n",
    "    right_attributes = aggdata2.X\n",
    "    right_tablename = aggdata2.name\n",
    "    \n",
    "    agg1 = agg1[[\"cov:s:\" + y, \"cov:Q:\" + y + \",\" + y, \"cov:c\"]]\n",
    "    \n",
    "    kept_cols = []\n",
    "    for col in right_attributes:\n",
    "        kept_cols.append(\"cov:s:\" + col)\n",
    "        kept_cols.append(\"cov:Q:\" + col + \",\" + col)\n",
    "        \n",
    "    agg2 = agg2[kept_cols]\n",
    "    \n",
    "    # wheter join on index\n",
    "    if left_inp:\n",
    "        # if it's a tuple\n",
    "        if isinstance(dimension, str):\n",
    "            join = pd.merge(agg1.set_index(dimension), agg2, how=how, left_index=True, right_index=True)\n",
    "        else:\n",
    "            join = pd.merge(agg1.set_index(list(dimension)), agg2, how=how, left_index=True, right_index=True)\n",
    "    else:\n",
    "        join = pd.merge(agg1, agg2, how=how, left_index=True, right_index=True)\n",
    "    \n",
    "    right_cov = aggdata2.covariance\n",
    "    \n",
    "    # fill in nan\n",
    "    for att2 in right_attributes:\n",
    "        join['cov:s:' + att2].fillna(value=right_cov['cov:s:' + att2], inplace=True)\n",
    "        join['cov:s:' + att2] *= join['cov:c']\n",
    "        \n",
    "    \n",
    "    if average:\n",
    "        for col in right_attributes:\n",
    "            join['cov:Q:' + col + \",\" + col].fillna(value=right_cov['cov:s:' + col]*right_cov['cov:s:' + col], inplace=True)\n",
    "            join['cov:Q:' + col + \",\" + col] *= join['cov:c']\n",
    "    \n",
    "    for att1 in left_attributes:\n",
    "        for att2 in right_attributes:\n",
    "            if 'cov:Q:' + att1 + \",\" + att2 in join:\n",
    "                join['cov:Q:' + att1 + \",\" + att2] = join['cov:s:' + att1] * join['cov:s:' + att2]/join['cov:c']\n",
    "            else:\n",
    "                join['cov:Q:' + att2 + \",\" + att1] = join['cov:s:' + att2] * join['cov:s:' + att1]/join['cov:c']\n",
    "    \n",
    "    return join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035f43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m79",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m79"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
